# -*- coding: utf-8 -*-
"""RAG-FitnessRL-Art.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jGVeaH9LZZYDZXkVwlaodDIGM3E3g9R

To train this email search agent using LangGraph, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU!

<div class="align-center">
<a href="https://github.com/openpipe/art"><img src="https://github.com/openpipe/art/raw/main/assets/ART_pill.png" height="50"></a>
<a href="https://discord.gg/zbBHRUpwf4"><img src="https://github.com/openpipe/art/raw/main/assets/Discord.png" height="50"></a>
<a href="https://art.openpipe.ai"><img src="https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png" height="50"></a>

Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).

</div>

<a href="https://art.openpipe.ai/"><img src="https://github.com/openpipe/art/raw/main/assets/Header_separator.png" height="5"></a>

**Fitness Agent with LangGraph**

In this notebook, you will be using [ART](https://github.com/openpipe/art) together with [LangGraph](https://langchain-ai.github.io/langgraph/) to train your own Fitness agent from scratch! This implementation demonstrates how to integrate LangGraph's agent framework with ART's training capabilities.

Beginning with a Qwen 2.5 7B base model, you will train it to generate full Nutrition and Workout plans using LangGraph's ReAct agent pattern. You will construct an [agentic environment](#Environment), define a [rollout](#Rollout) using LangGraph, and run a [training loop](#Loop). You will also learn how to use [RULER](#ruler) to judge the quality of the agent's answers and how to build verifiable reward functions.

**RULER**

RULER is a robust technique for evaluating the quality of an agent's answers and training the agent to produce more of its best completions. To learn more about RULER, see the [RULER documentation](https://art.openpipe.ai/fundamentals/ruler).



Now let's get started!
"""

# Commented out IPython magic to ensure Python compatibility.
# This mounts your Google Drive to the Colab VM.
from google.colab import drive
drive.mount('/content/drive')

# TODO: Enter the foldername in your Drive where you have saved the unzipped
FOLDERNAME =  "AI/AgentRL/FitnessRL"
assert FOLDERNAME is not None, "[!] Enter the foldername."

# Now that we've mounted your Drive, this ensures that
# the Python interpreter of the Colab VM can load
# python files from within it.
import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))
# %cd /content/drive/My\ Drive/$FOLDERNAME/
!ls -a

# Commented out IPython magic to ensure Python compatibility.
# #@title ðŸ’¿ Installation
# 
# # Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)
# # Copyright (c) Unsloth contributors.
# # License: GNU LGPL v3.0.
# # Modifications by OpenPipe:
# # - switched to uv
# # - changed vllm/triton pinning logic
# # - added litellm/protobuf pins
# # See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.
# 
# %%capture
# import os
# 
# if "COLAB_" not in "".join(os.environ.keys()):
#     !uv pip install openpipe-art[backend,langgraph] langchain-core langgraph langchain_openai tenacity datasets pinecone  --prerelease allow --no-cache-dir
# else:
#     try:
#         import numpy
# 
#         get_numpy = f"numpy=={numpy.__version__}"
#     except:
#         get_numpy = "numpy"
#     try:
#         import subprocess
# 
#         is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
#     except:
#         is_t4 = False
#     get_vllm, get_triton = (
#         ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm", "triton")
#     )
#     !uv pip install --upgrade \
#         openpipe-art[backend,langgraph] langchain-core langgraph langchain_openai tenacity datasets protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir
#     !uv pip install -qqq {get_triton}

import os

from dotenv import load_dotenv

load_dotenv()

# Required
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

from google.colab import userdata

# Get the API key from Colab secrets
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
os.environ["WANDB_API_KEY"] = userdata.get('wandb')
os.environ["PINECONE_API_KEY"] = userdata.get('PINECONE_API_KEY')

if not os.environ.get("OPENAI_API_KEY"):
    raise ValueError(
        "OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini."
    )

# Optional
# os.environ["WANDB_API_KEY"] = "YOUR_API_KEY"

if not os.environ.get("WANDB_API_KEY"):
    print("WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.")

import os
import random
import sqlite3
from dataclasses import asdict, dataclass
from datetime import datetime
from textwrap import dedent
from typing import List, Literal, Optional

from datasets import Dataset, Features, Sequence, Value, load_dataset
from pydantic import BaseModel, Field
from tqdm import tqdm

class Scenario(BaseModel):
   question: str
   split: Literal["train", "test"]
   id: str
   daily_cal_target: Optional[int] = None
   daily_prot_target: Optional[int] = None
   daily_carb_target: Optional[int] = None
   daily_fat_target: Optional[int] = None
   banned_keywords: Optional[List[str]] = None

from typing import Any, Dict, Literal, Union
from pydantic import field_validator



class FinalAnswer(BaseModel):
    answer: Dict[str, Any]

    @field_validator("answer", mode="before")
    @classmethod
    def ensure_dict(cls, v):
            # Unwrap nested FinalAnswer by mistake
            if isinstance(v, FinalAnswer):
                return v.answer
            # Parse JSON string
            if isinstance(v, str):
                try:
                    return json.loads(v)
                except json.JSONDecodeError as e:
                    raise ValueError(f"answer must be a JSON object string or dict; got invalid JSON: {e}")
            # Already a dict
            if isinstance(v, dict):
                return v
            raise TypeError(f"Unsupported type for answer: {type(v).__name__}")


@dataclass
class SearchResult:
    message_id: str
    snippet: str

from datasets import load_dataset

# Load the dataset from the JSONL file
dataset = load_dataset("json", data_files="fitness_scenarios.jsonl")

# Assuming the dataset has a "train" split, you can access it like this:
training_scenarios = dataset["train"]

print("Dataset loaded successfully!")
print(training_scenarios)

training_scenarios[-1]

import json

def one_day_meal_question(example):
    one_day_prompt = "generate a one day meal plan for user that match its macros and diet"
    example['input_question'] = f"{one_day_prompt}"
    return example

def combine_question_and_context(example):
    context_str = json.dumps(example['context'])
    example['question'] = f"{example['input_question']} Context: {context_str}"
    return example

def convert_val_to_test(example):
    if example['split'] == 'val':
       example['split'] = 'test'
    return example

def get_target_nutrition_data(example):
    daily_cal_target = example['context']['daily_cal_target']
    daily_prot_target = example['context']['daily_prot_target']
    daily_carb_target = example['context']['daily_carb_target']
    daily_fat_target = example['context']['daily_fat_target']
    banned_keywords = example['context']['banned_keywords']

    example['daily_cal_target'] = daily_cal_target
    example['daily_prot_target'] = daily_prot_target
    example['daily_carb_target'] = daily_carb_target
    example['daily_fat_target'] = daily_fat_target
    example['banned_keywords'] = banned_keywords
    return example

def extract_context_columns(example):
    context = example['context']
    example['age'] = context.get('age')
    example['sex'] = context.get('sex')
    example['height_cm'] = context.get('height_cm')
    example['weight_kg'] = context.get('weight_kg')
    example['goal'] = context.get('goal')
    example['activity'] = context.get('activity')
    example['dietary_prefs'] = context.get('dietary_prefs')
    example['equipment'] = context.get('equipment')
    example['experience'] = context.get('experience')
    return example

def make_scenario(example):
    scenario = Scenario(
        question=example['question'],
        split=example['split'],
        id=str(example['id']),
        daily_cal_target=example['daily_cal_target'],
        daily_prot_target=example['daily_prot_target'],
        daily_carb_target=example['daily_carb_target'],
        daily_fat_target=example['daily_fat_target'],
        #banned_keywords=example['banned_keywords']
    )
    return scenario
training_scenarios = training_scenarios.map(one_day_meal_question)
training_scenarios = training_scenarios.map(combine_question_and_context)
training_scenarios = training_scenarios.map(convert_val_to_test)
training_scenarios = training_scenarios.map(get_target_nutrition_data)
training_scenarios = training_scenarios.map(extract_context_columns)
# training_scenarios = training_scenarios.map(make_scenario)
print(training_scenarios[0]['question'])

scenarios_list = []
for example in training_scenarios:
    scenario = Scenario(
        question=example['question'],
        split=example['split'],
        id=str(example['id']),
        daily_cal_target=example['daily_cal_target'],
        daily_prot_target=example['daily_prot_target'],
        daily_carb_target=example['daily_carb_target'],
        daily_fat_target=example['daily_fat_target'],
        banned_keywords=example['banned_keywords']
    )
    scenarios_list.append(scenario)

print(f"Created a list of {len(scenarios_list)} Scenario objects.")

from collections import Counter

print(Counter(training_scenarios['split']))

scenarios_list[0]

!uv pip install Pillow==10.3.0

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall -y pillow PIL
# %pip install --upgrade --force-reinstall --no-cache-dir "Pillow>=10.3.0,<12"
# Then restart the kernel/runtime

import art
from art.local import LocalBackend

random.seed(42)

base_model_name = "Qwen/Qwen2.5-7B-Instruct"
#base_model_name= "Qwen/Qwen3-4B-Instruct-2507"

model = art.TrainableModel(
    name="fitness-agent-langgraph-7B-qwen2.5-004",
    project="fitness-agent-langgraph-rag",
    base_model=base_model_name,
)

model._internal_config = art.dev.InternalModelConfig(
    init_args=art.dev.InitArgs(
        max_seq_length=8192,
    ),
    engine_args=art.dev.EngineArgs(
        enforce_eager=True,
        gpu_memory_utilization=0.8,
    ),
)

# Initialize the server
backend = LocalBackend(
    # Normally we don't want to run the server in-process, but for the output
    # to show up properly on Google Colab we'll enable this.
    in_process=True,
    path="./.art",
)

# Register the model with the local Backend (sets up logging, inference, and training)
await model.register(backend)

RULER_JUDGE_PROMPT = """
You are a strict evaluator for weekly nutrition/workout plans for SyntraFit.

Score each ASSISTANT response on 0â€“1 (decimals allowed) based on:

[Critical]
1) STRUCTURE: Valid JSON structure per task .
2) TARGET FIT: Daily calories & protein within Â±5% of user's target_nutrition_data (if nutrition task).
3) DIETARY RULES: No banned items present (e.g., â€œeggâ€, â€œshellfishâ€) if user disallows.
4) TOOLING LOGIC: The content is plausibly the result of correct tools (recipe_semantic_search for recipes; get_available_exercises for exercises).

[Quality]
6) REALISM: Meals/exercises are realistic, balanced, and varied across the week; sequencing makes sense (snacks vs meals; rest days).
7) CLARITY: Names are specific (e.g., â€œGrilled Salmon & Quinoaâ€ not â€œhealthy bowlâ€).

Return ONLY a float in [0,1]. Heavily penalize if structure or targets are wrong.
"""

MAX_TURNS = 30

PLANNER_PROMPT = f"""
You are a nutrition planner specialist who creates daily nutrition plans. Think carefully and pay great attention to macro numbers.

You must create a one-day meal plan that meets the user's macro and dietary targets.

TOOLS YOU CAN USE (names must match exactly):
1) recipe_semantic_search(meal_query, k) â€“ Search for relevant recipes and return their true macros.
2) return_final_answer_tool(answer) â€“ Return the final answer (JSON plan).

ROUTING RULES (VERY IMPORTANT):
- If the user asks for a meal/nutrition plan, you MUST NOT call get_available_exercises or generate_workout_plan_mutation.
- Even if the request is for a 7-day plan, create only a single-day meal plan.

EXECUTION POLICY:
- You MAY call tools during reasoning to gather information and choose recipes.
- The FINAL assistant message must output ONLY a single call to return_final_answer_tool with the exact JSON plan (stringified if needed).
- You may take up to {MAX_TURNS} turns to find the answer.

============================================================
NUTRITION PLAN PIPELINE
============================================================

1) PLAN SKELETON
   â€¢ Generate a one-day meal plan for the user. The plan should have meals that fulfill the user's daily macro targets.
   â€¢ Create a reasonable number of meals (meals can include snacks). Base the count/portions on the user's macro targets.
   â€¢ Meal names in the final plan MUST be recipe names returned by recipe_semantic_search (do not invent names).

2) USE RECIPE SEMANTIC SEARCH TOOL
   â€¢ Use recipe_semantic_search to find relevant recipes with correct nutrition info.
   â€¢ You MUST ALWAYS use the macros retrieved from the tool and NOT infer your own data.
   â€¢ For each meal you include, set (calories, proteins, carbs, fats) to k Ã— the tool macros for that chosen recipe (k may be fractional). Do not fabricate numbers.
   â€¢ If a candidate recipe includes any banned keywords/ingredients from context, discard it and search again.

3) MACRO ADJUSTMENT (per day)
   â€¢ Sum macros for the day.
   â€¢ If totals differ from the user's daily targets, swap recipes or adjust the multiplier k (still using tool macros) until daily totals are within Â±5% of the user's targets (calories/protein/carbs/fat).
   â€¢ Respect ALL banned keywords/ingredients from context.

4) JSON MEAL PLAN (scratch-step)
   â€¢ Build JSON matching this schema (no comments):

     {{
       "meals": [
         {{
           "name": "Grilled Chicken & Rice",
           "calories": 700,
           "proteins": 45,
           "carbs": 60,
           "fats": 20,
           "sequence": 1
         }}
       ]
     }}

   â€¢ Ensure the summed macros for the day are within Â±5% of the targets.
   â€¢ IMPORTANT: Every "name" MUST match a recipe name previously returned by recipe_semantic_search. Macros MUST be exact multiples of that recipe's macros.

5) IF YOU REACHED MAX_TURNS and you have not found a final answer, return the best final answer you can with all information you gathered.

6) TOOL CALL (FINALIZE)
   â€¢ Call return_final_answer_tool with:
     - answer = the EXACT JSON meal plan (stringified if needed)

"""

!pip install pinecone

from pinecone import Pinecone
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

recipe_index_name = "syntrafit-recipes-nutrition"
exercise_index_name = "syntrafit-exercises"

recipe_index = pc.Index(recipe_index_name)
exercise_index = pc.Index(exercise_index_name)

def extract_meal_names(data):
    data = data['result']
    return_data = []


    return [{"id" : hit['_id'] ,"name": hit["fields"]["name"], "calories":  hit["fields"]["calories"],"carbs":  hit["fields"]["carbohydrates"], "protein": hit["fields"]["protein"], "fat":  hit["fields"]["fat"]}  for hit in data["hits"] if "fields" in hit and "name" in hit["fields"]]

 # Search the dense index
results = recipe_index.search(
          namespace="syntrafit",
          query={
              "top_k": 2,
              "inputs": {
                  'text': " Chicken and rice healthy"
              }
          },
          rerank={
          "model": "bge-reranker-v2-m3",
          "top_n": 2,
          "rank_fields": [ "name"]
    },
      )

print(results)

extract_meal_names(results)

import json
import art
from art.rewards import ruler_score_group
import uuid

import weave
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from litellm import acompletion
from tenacity import retry, stop_after_attempt
from art.langgraph import init_chat_model


if os.getenv("WANDB_API_KEY", ""):
    weave.init(model.project, settings={"print_call_link": False})


class NutritionJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    score: float = Field(description="Score between 0 and 1.")


@retry(stop=stop_after_attempt(3))
async def nutrition_judge_score(scenario, answer):


    # # 2) RULER judge
    # judged = await ruler_score_group(
    #     group, "openai/o4-mini",
    #     system_instruction=RULER_JUDGE_PROMPT, debug=False
    # )

    messages = [
        {"role": "system", "content": RULER_JUDGE_PROMPT},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"

                f"Assistant Answer: {answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model="openai/gpt-4.1",
        messages=messages,
        response_format=NutritionJudgeResponse,
    )

    # Access attributes directly instead of using .get()
    return NutritionJudgeResponse(reasoning=response.choices[0].message.content.reasoning, score=response.choices[0].message.content.score)

import json
from dataclasses import is_dataclass, asdict

def _extract_first_json_segment(s: str) -> str | None:
    """Extract the first complete top-level JSON object or array from a noisy string."""
    start_candidates = [s.find('{'), s.find('[')]
    start_candidates = [i for i in start_candidates if i != -1]
    if not start_candidates:
        return None
    start = min(start_candidates)

    # Track either {} or [] depth; handle strings and escapes
    depth_obj = 0
    depth_arr = 0
    in_str = False
    esc = False

    opener = s[start]
    want_obj = opener == '{'
    want_arr = opener == '['

    for i in range(start, len(s)):
        ch = s[i]
        if in_str:
            if esc:
                esc = False
            elif ch == '\\':
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == '{':
                depth_obj += 1
            elif ch == '}':
                depth_obj -= 1
            elif ch == '[':
                depth_arr += 1
            elif ch == ']':
                depth_arr -= 1

            # Completed top-level
            if want_obj and depth_obj == 0 and depth_arr == 0 and i >= start:
                return s[start:i+1]
            if want_arr and depth_arr == 0 and depth_obj == 0 and i >= start:
                return s[start:i+1]
    return None  # unbalanced / truncated

def _loads_loose(s: str):
    """Try multiple strategies to convert a string into JSON (object or array)."""
    # 1) Try direct
    try:
        v = json.loads(s)
    except json.JSONDecodeError:
        v = None

    # 2) If failed OR result is a string (double-encoded), try to decode up to 3 times
    tries = 0
    while isinstance(v, str) and tries < 3:
        tries += 1
        try:
            v = json.loads(v)
        except json.JSONDecodeError:
            break

    if v is not None and not isinstance(v, str):
        return v

    # 3) Try to extract first clean JSON segment from noisy logs and parse it
    seg = _extract_first_json_segment(s)
    if seg is not None:
        try:
            return json.loads(seg)
        except json.JSONDecodeError:
            pass

    # Give up
    raise json.JSONDecodeError("Could not parse JSON from string", s, 0)

def get_payload(obj):
    """Return a dict payload from FinalAnswer/str/dict.
    - Accepts dicts and top-level arrays (wrapped under '_root')
    - Unwraps .answer
    - Parses double-encoded JSON strings
    - Extracts JSON from noisy/log-polluted strings
    - Supports Pydantic and dataclasses
    """
    # unwrap FinalAnswer wrapper
    if hasattr(obj, "answer"):
        obj = obj.answer

    # decode bytes
    if isinstance(obj, (bytes, bytearray)):
        obj = obj.decode("utf-8", errors="replace")

    # parse JSON string (robust)
    if isinstance(obj, str):
        try:
            obj = _loads_loose(obj)
        except json.JSONDecodeError:
            return {"_error": "invalid_json_string", "_raw": obj}

    # pydantic compatibility
    if hasattr(obj, "model_dump"):
        obj = obj.model_dump()
    elif hasattr(obj, "dict"):
        try:
            obj = obj.dict()
        except TypeError:
            # some dataclasses also have .dict attribute conflicts; handle below
            pass

    # dataclasses
    if is_dataclass(obj):
        obj = asdict(obj)

    # final shape handling
    if isinstance(obj, dict):
        return obj
    if isinstance(obj, list):  # allow top-level arrays while keeping a dict payload
        return {"_root": obj}

    return {"_error": f"unexpected_type:{type(obj).__name__}", "_raw": str(obj)}

# === Provenance reward (names-only, totals-only) ===
# Matches plan meals to recipe_semantic_search results by NAME only
# and verifies totals are a consistent multiple of tool macros.

import json, math, re, difflib
from statistics import median

# ---------- utils ----------
def _norm_name(s: str) -> str:
    if not isinstance(s, str):
        return ""
    # collapse spaces, remove trivial punctuation (& , .  multiple spaces)
    s = s.lower()
    s = re.sub(r"[&]", " and ", s)
    s = re.sub(r"[^a-z0-9\s]", " ", s)           # keep only letters/digits/spaces
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _rel_err(true, pred):
    if true == 0:
        return 0.0 if abs(pred) < 1e-6 else 1.0
    return abs(pred - true) / max(1.0, abs(true))

def _infer_multiplier(unit: dict, totals: dict, min_dims=2):
    """
    Infer multiplier m from totals using median of ratios across available macros.
    Requires at least `min_dims` macros present. Returns (m, worst_rel_err) or (None, None).
    """
    ratios, dims = [], []
    for k in ("calories", "carbs", "protein", "fat"):
        t = totals.get(k)
        u = unit.get(k)
        if t is None or u in (None, 0):
            continue
        try:
            ratios.append(float(t) / float(u))
            dims.append(k)
        except Exception:
            pass
    if len(ratios) < min_dims:
        return None, None
    m = median(ratios)
    if not (m > 0 and math.isfinite(m)):
        return None, None
    worst = 0.0
    for k in dims:
        pred = unit[k] * m
        err = _rel_err(pred, float(totals[k]))
        if err > worst:
            worst = err
    return m, worst

def _flatten_plan_meals(payload: dict):
    """Expect schema:
    {
      "meals": [
        {"name": "...", "calories": ..., "proteins": ..., "carbs": ..., "fats": ..., "sequence": ...},
        ...
      ]
    }
    """
    meals = []
    if isinstance(payload, dict) and isinstance(payload.get("meals"), list):
        for m in payload["meals"]:
            if isinstance(m, dict):
                meals.append(m)
    return meals

def _extract_totals_from_meal(meal: dict):
    """Map plan keys -> canonical keys (proteins->protein, fats->fat)."""
    totals = {}
    if not isinstance(meal, dict):
        return totals
    if "calories" in meal: totals["calories"] = meal["calories"]
    if "carbs"    in meal: totals["carbs"]    = meal["carbs"]
    if "proteins" in meal: totals["protein"]  = meal["proteins"]
    if "fats"     in meal: totals["fat"]      = meal["fats"]
    return totals

# ---------- catalog from tool logs ----------
def _collect_catalog_from_logs_by_name(traj, tool_name="recipe_semantic_search"):
    """
    Build {normalized_name: {"raw_name": str, "macros": {calories,carbs,protein,fat}, "id": <id>}}
    from tool logs where result looks like:
      [
        {"id": "...", "name": "...", "calories": 382.0, "carbs": 35.0, "protein": 33.0, "fat": 12.0},
        ...
      ]
    or {"recipes": [ ...same list... ]}
    """
    catalog = {}
    for m in getattr(traj, "messages_and_choices", []):
        if m.get("role") != "tool_log":
            continue
        try:
            log = json.loads(m["content"])
        except Exception:
            continue
        end = log.get("end")
        if not end or end.get("tool") != tool_name:
            continue

        res = end.get("result")
        # result might be a JSON string or a Python list/dict
        if isinstance(res, str):
            try:
                res = json.loads(res)
            except Exception:
                pass

        recipes = res.get("recipes") if isinstance(res, dict) else res
        if not isinstance(recipes, list):
            continue

        for r in recipes:
            name = r.get("name")
            if not name:
                continue
            key = _norm_name(name)
            try:
                catalog[key] = {
                    "raw_name": name,
                    "id": r.get("id"),
                    "macros": {
                        "calories": float(r["calories"]),
                        "carbs": float(r["carbs"]),
                        "protein": float(r["protein"]),
                        "fat": float(r["fat"]),
                    },
                }
            except Exception:
                # skip incomplete/bad rows
                continue
    return catalog

# ---------- main reward ----------
def provenance_reward_names_only_totals_only(
    payload: dict,
    traj,
    tol_pct: float = 0.05,
    name_match_cutoff: float = 0.92,
    tool_name: str = "recipe_semantic_search",
):
    """
    Returns (score_in_[0,1], info_dict).

    Pass criteria per meal:
      1) Meal name must match (exact or fuzzy) a recipe name returned by the tool
         during this trajectory.
      2) Meal's totals (calories, carbs, proteins, fats) must be a consistent
         multiple of the tool's macros (worst relative error <= tol_pct).

    No 'servings', no per-serving fields, names-only linkage.
    """
    catalog = _collect_catalog_from_logs_by_name(traj, tool_name=tool_name)
    if not catalog:
        return 0.0, {"reason": "no_recipe_tool_usage_detected"}

    meals = _flatten_plan_meals(payload)
    if not meals:
        return 0.0, {"reason": "empty_plan"}

    checked = passed = 0
    details = []

    # precompute name list for fuzzy matching
    cat_names = list(catalog.keys())

    for meal in meals:
        checked += 1
        plan_name_raw = meal.get("name", "")
        plan_key = _norm_name(plan_name_raw)

        # Try exact normalized match first
        match_key = plan_key if plan_key in catalog else None

        # Fallback: fuzzy match on normalized names
        if match_key is None and cat_names:
            best = difflib.get_close_matches(plan_key, cat_names, n=1, cutoff=name_match_cutoff)
            if best:
                match_key = best[0]

        if match_key is None:
            details.append({
                "ok": False,
                "plan_name": plan_name_raw,
                "reason": "no_name_match_in_tool_results",
            })
            continue

        canon = catalog[match_key]["macros"]
        totals = _extract_totals_from_meal(meal)
        if not totals:
            details.append({
                "ok": False,
                "plan_name": plan_name_raw,
                "matched_recipe_name": catalog[match_key]["raw_name"],
                "reason": "no_totals_in_meal",
            })
            continue

        m, worst_err = _infer_multiplier(canon, totals, min_dims=2)
        ok = (m is not None and worst_err is not None and worst_err <= tol_pct)

        details.append({
            "ok": ok,
            "plan_name": plan_name_raw,
            "matched_recipe_name": catalog[match_key]["raw_name"],
            "matched_recipe_id": catalog[match_key]["id"],
            "inferred_multiplier": m,
            "worst_rel_err": worst_err,
        })

        if ok:
            passed += 1

    score = passed / max(1, checked)
    return score, {"checked": checked, "passed": passed, "items": details}

# ---------- minimal demo (optional) ----------
if __name__ == "__main__":
    class DummyTraj:
        def __init__(self):
            self.messages_and_choices = []

    # Simulate a tool_log "end" with your exact shape
    tool_result = [
        {'id': '39461',  'name': 'Quick Chicken And Rice Salad  Low Fat',            'calories': 382.0, 'carbs': 35.0, 'protein': 33.0, 'fat': 12.0},
        {'id': '131802', 'name': 'Quick   Easy Chicken  Broccoli And Brown Rice',    'calories': 379.0, 'carbs': 33.0, 'protein': 30.0, 'fat': 13.0},
    ]

    traj = DummyTraj()
    traj.messages_and_choices.append({
        "role": "tool_log",
        "content": json.dumps({
            "end": {
                "tool": "recipe_semantic_search",
                "kwargs": {"meal_query": "chicken rice"},
                "result": json.dumps(tool_result)  # logger stores JSON string
            }
        })
    })

    # Plan payload respecting your schema (totals only)
    payload = {
        "meals": [
            {
                "name": "Quick Chicken And Rice Salad Low Fat (x2)",
                "calories": 382.0*2, "proteins": 33.0*2, "carbs": 35.0*2, "fats": 12.0*2,
                "sequence": 1
            },
            {
                # Slight name differences and extra spaces should still match
                "name": "Quick & Easy Chicken, Broccoli and Brown Rice",
                "calories": 379.0, "proteins": 30.0, "carbs": 33.0, "fats": 13.0,
                "sequence": 2
            },
            {
                # No match in tool results -> fail
                "name": "Greek Yogurt with Nuts",
                "calories": 250.0, "proteins": 20.0, "carbs": 15.0, "fats": 10.0,
                "sequence": 3
            },
        ]
    }

    score, info = provenance_reward_names_only_totals_only(payload, traj, tol_pct=0.05, name_match_cutoff=0.9)
    print("Provenance score:", round(score, 4))
    print("Checked:", info["checked"], "Passed:", info["passed"])
    for i, row in enumerate(info["items"], 1):
        print(f"{i}. ok={row['ok']} | plan='{row.get('plan_name')}' "
              f"| matched='{row.get('matched_recipe_name')}' "
              f"| m={row.get('inferred_multiplier')} | worst_err={row.get('worst_rel_err')}")

from src.helpers import get_exercise_list_for_prompt
from src.env.verifiers_utils import verify_nutrition_plan, verify_workout_plan, verify_nutrition_schema, verify_meal_plan_schema, verify_daily_meal_plan_macros, is_valid_json
from src.data_utils.create_synthetic_data import data
from src.env.verifiable_rewards.nutrition_rewards import nutrition_reward

import random
from typing import Dict, Any, Tuple

def combine_reward_grpo(
    nutri: float,                 # macros/target score (any scale; we'll clamp)
    prov: float,                  # provenance score in [0,1]
    step: int | None = None,      # training step for scheduling (optional)
    gamma: float = 1.0,           # emphasis on macros
    beta: float | None = None,    # emphasis on provenance (if None -> schedule)
    eps: float = 0.03,            # floor to avoid zero-collapse in the gate
    mix: float = 0.15,            # how much dense linear part to mix in (0..0.3 works well)
    jitter: float = 1e-3,         # tiny tie-breaker within a group
) -> Tuple[float, Dict[str, Any]]:
    """
    Returns (score_for_grpo, info). Score is in [0,1].
    Designed for GRPO: adds a small dense component and jitter to preserve ranking/spread.
    """

    # ---- simple schedule for beta (increase provenance pressure over time) ----
    def _beta_schedule(s: int | None) -> float:
        if s is None:  return 1.5
        if s < 100:    return 1.2
        if s < 300:    return 1.5
        return 1.8

    B = beta if beta is not None else _beta_schedule(step)

    # ---- clamp and apply epsilon floor for the gate ----
    n = max(0.0, min(1.0, float(nutri)))
    p = max(0.0, min(1.0, float(prov)))
    n_gate = max(eps, n)
    p_gate = max(eps, p)

    # ---- sparse gate (soft-AND) ----
    gate = (n_gate ** gamma) * (p_gate ** B)
    # normalize gate to [0,1] given eps floor
    gate_min = (eps ** (gamma + B))
    if gate_min < 1.0:
        gate = (gate - gate_min) / (1.0 - gate_min)
    gate = max(0.0, min(1.0, gate))

    # ---- dense part to keep intra-group spread (rank signal even when gate fails) ----
    # small linear mix that still prefers provenance
    dense = 0.6 * p + 0.4 * n    # both already in [0,1]

    # ---- final blend + jitter (rank only; GRPO will mean-center anyway) ----
    score = (1.0 - mix) * gate + mix * dense + random.random() * jitter
    score = max(0.0, min(1.0, score))

    info = {
        "nutri_clamped": n, "prov_clamped": p,
        "gamma": gamma, "beta": B, "eps": eps,
        "mix": mix, "jitter": jitter,
        "gate": gate, "dense": dense, "score": score
    }
    return score, info

import json
from functools import wraps
from datetime import datetime

import uuid

import weave
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from litellm import acompletion
from tenacity import retry, stop_after_attempt
from art.langgraph import init_chat_model

import art

if os.getenv("WANDB_API_KEY", ""):
    weave.init(model.project, settings={"print_call_link": False})

# MAX_TURNS = 20

class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None

class FitnessScenario(BaseModel):
    step: int
    scenario: Scenario


@weave.op
async def rollout(model: art.Model, fitness_scenario: FitnessScenario) -> ProjectTrajectory:
    scenario = fitness_scenario.scenario

    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
        metadata={
            "scenario_id": str(scenario.id),
            "step": fitness_scenario.step,
        },
    )

    system_prompt = dedent(
        PLANNER_PROMPT
    )

    # Store final answer in trajectory
    final_answer = None

    # Target Nutrition
    daily_cal_target = scenario.daily_cal_target
    daily_prot_target = scenario.daily_prot_target
    daily_carb_target = scenario.daily_carb_target
    daily_fat_target = scenario.daily_fat_target


    # def log_tool(tool_name):
    #     def decorator(fn):
    #         @wraps(fn)
    #         def wrapper(*args, **kwargs):
    #             call = {
    #                 "tool": tool_name,
    #                 "ts": datetime.utcnow().isoformat(),
    #                 "args": args, "kwargs": kwargs
    #             }
    #             print(f"[TOOL START] {tool_name} args={kwargs}")
    #             traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"start": call})})
    #             try:
    #                 out = fn(*args, **kwargs)
    #                 print(f"[TOOL END] {tool_name} result_preview={str(out)[:400]}")
    #                 traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"end": {**call, "result": out}})})
    #                 return out
    #             except Exception as e:
    #                 print(f"[TOOL ERROR] {tool_name}: {e}")
    #                 traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"error": {**call, "error": str(e)}})})
    #                 raise
    #         return wrapper
    #     return decorator


    def log_tool(tool_name):
        def decorator(fn):
            @wraps(fn)
            def wrapper(*args, **kwargs):
                GREEN = "\033[92m"
                RESET = "\033[0m"

                call = {
                    "tool": tool_name,
                    "ts": datetime.utcnow().isoformat(),
                    "args": args,
                    "kwargs": kwargs
                }

                color_prefix = GREEN if "final_answer" in tool_name else ""
                color_reset = RESET if "final_answer" in tool_name else ""

                print(f"{color_prefix}[TOOL START] {tool_name} args={kwargs}{color_reset}")
                traj.messages_and_choices.append(
                    {"role": "tool_log", "content": json.dumps({"start": call})}
                )

                try:
                    out = fn(*args, **kwargs)
                    print(f"{color_prefix}[TOOL END] {tool_name} result_preview={str(out)[:400]}{color_reset}")
                    traj.messages_and_choices.append(
                        {"role": "tool_log", "content": json.dumps({"end": {**call, 'result': out}})}
                    )
                    return out
                except Exception as e:
                    print(f"\033[91m[TOOL ERROR] {tool_name}: {e}\033[0m")
                    traj.messages_and_choices.append(
                        {"role": "tool_log", "content": json.dumps({"error": {**call, "error": str(e)}})}
                    )
                    raise
            return wrapper
        return decorator

    @tool
    @log_tool("recipe_semantic_search")
    def recipe_semantic_search(meal_query: str, k: int = 5) -> str:
      """Search the recipe index for the most similar recipes to the query."""
      # Search the dense index
      results = recipe_index.search(
          namespace="syntrafit",
          query={
              "top_k": k,
              "inputs": {
                  'text': meal_query
              }
          }
      )

      results = extract_meal_names(results)

      print(results)

      return results


    @tool
    @log_tool("return_final_answer_tool")
    def return_final_answer_tool(answer: str) -> dict:
        """Return the final answer (daily meal plan) in the correct format """
        nonlocal final_answer
        payload = get_payload(answer)          # <-- normalize here
        final_answer = FinalAnswer(answer=payload)
        return final_answer.model_dump()


    # Create LangGraph tools
    tools = [ recipe_semantic_search,  return_final_answer_tool] #recipe_semantic_search,

    # Pass the local path to the model
    chat_model = init_chat_model(f"{model.name}", temperature=0.5)


    # Create the LangGraph ReAct agent
    react_agent = create_react_agent(chat_model, tools)
    print("LangGraph agent created!")
    print(react_agent)

    try:
        # Run the agent
        config = {
            "configurable": {"thread_id": str(uuid.uuid4())},
            "recursion_limit": MAX_TURNS,
        }

        print("Human Question:", scenario.question )



        # Run the agent to get the final result
        res =await react_agent.ainvoke(
            {
                "messages": [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=scenario.question),
                ]
            },
            config=config,
        )


        print("rollout_2")
        print(res)

        # Check if we got a final answer
        if final_answer:
            print("Got final answer!")
            print(final_answer)

            payload = get_payload(final_answer)    # <-- normalize again defensively
            print(payload)                         # should print a dict, not a string

            # Calculate the total reward
            total_reward = 0.0
            traj.final_answer = final_answer
            print(final_answer.answer)

            banned_keywords = scenario.banned_keywords
            print(banned_keywords)


            reward, nutri_info = nutrition_reward(
                      payload,
                      daily_cal_target=daily_cal_target,
                      daily_prot_target=daily_prot_target,
                      banned_keywords=banned_keywords,
                      daily_carb_target=daily_carb_target,    # optional
                      daily_fat_target=daily_fat_target,      # optional
                      step=fitness_scenario.step,             # enables annealing
            )

            print(f"Nutrition reward: {reward}")
            print(f"Info: {nutri_info}")

            prov_score, prov_info = provenance_reward_names_only_totals_only(payload, traj, tol_pct=0.07)
            print(f"Provenance reward: {prov_score}")
            print(f"Info: {prov_info}")

            # # Combine (choose weights; example below)
            # W_MACROS_SCHEMA   = 0.7
            # W_PROVEN   = 0.3   # NEW: encourages using tool results faithfully

            # total_reward += (W_MACROS_SCHEMA * reward) + (W_PROVEN * prov_score)

            # Emphasize provenance slightly with an exponent beta > 1
            BETA = 1.5   # try 1.3â€“2.0
            GAMMA = 1.0  # macro emphasis; increase to >1 if you want tighter macro pressure

            total_reward = (reward ** GAMMA) * (prov_score ** BETA)

            traj.metrics["macros_schema"] = reward
            traj.metrics["provenance"] = prov_score

            macros_schema_score = reward   # use the SAME metric you plotted for macros/schema
            provenance_score    = prov_score     # the one you plotted for provenance

            score, info = combine_reward_grpo(
                nutri=macros_schema_score,
                prov=provenance_score,
                step=getattr(fitness_scenario, "step", None),
                # gamma=1.0,      # usually fine
                # beta=None,      # let schedule drive it
                # eps=0.03, mix=0.15, jitter=1e-3  # default values
            )

            traj.reward = score
            print(f"Combined reward: {score}")
            print(f"Info: {info}")
            #traj.metrics["reward_info"] = info



            # traj.final_answer = final_answer
            # # Score the trajectory
            # correctness_judge_response = await nutrition_judge_score(
            #     scenario, traj.final_answer.answer
            # )

            # judge_score= correctness_judge_response.score
            # print("Judge score:", judge_score)
            # total_reward = total_reward + judge_score

            # judge_reasoning = correctness_judge_response.reasoning
            # print("Judge reasoning:", judge_reasoning)
            # traj.metrics["judge_reasoning"] = judge_reasoning



            random_noise = random.random()
            #traj.reward = total_reward + random_noise * 0.005
            print(f"Total reward: {traj.reward}")
            traj.metrics["correct"] = score

        # traj.reward= -1.0 + random_noise * 0.005
        # traj.metrics["correct"] = traj.reward


    except Exception as e:
        print(f"Error running LangGraph agent: {e}")
        # Add error information to trajectory
        traj.messages_and_choices.append(
            {"role": "assistant", "content": f"Error: {str(e)}"}
        )

    return traj


print("LangGraph rollout function defined!")

# Training configuration
from art.utils import iterate_dataset
from art.langgraph import wrap_rollout

training_config = {
    "groups_per_step": 4,
    "num_epochs": 3,
    "rollouts_per_group": 8,
    "learning_rate": 1e-5,
    "max_steps": 150,
}

# Use iterate_dataset with real training scenarios (similar to train.py)
training_iterator = iterate_dataset(
    scenarios_list,  # Use real scenarios from Hugging Face , training_scenarios
    groups_per_step=training_config["groups_per_step"],
    num_epochs=training_config["num_epochs"],
    #initial_step=await model.get_step(),
)

for batch in training_iterator:
    print(
        f"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}"
    )
    print(f"Batch contains {len(batch.items)} scenarios")

    # Create trajectory groups for this batch (similar to train.py)
    groups = []
    for scenario_data in batch.items:
        # # Ensure the id is a string when creating FitnessScenario
        # scenario = Scenario(
        #     question=scenario_data['question'],
        #     split=scenario_data['split'],
        #     id=str(scenario_data['id']) # Convert id to string here
        # )
        scenario = scenario_data
        print(scenario)
        groups.append(
            art.TrajectoryGroup(
                (
                    wrap_rollout(model, rollout)(
                        #model, FitnessScenario(step=batch.step, scenario=scenario.model_dump())
                        model, FitnessScenario(step=batch.step, scenario=scenario.model_dump())
                    )
                    for _ in range(training_config["rollouts_per_group"])
                )
            )
        )
    print("Group:", groups[0])
    # Gather all trajectory groups
    finished_groups = await art.gather_trajectory_groups(
        groups,
        pbar_desc="gather",
        max_exceptions=training_config["rollouts_per_group"] * len(batch.items),
    )

    print("Finished groups:", finished_groups)

    # judged_groups = []
    # for group in finished_groups:
    #     # Use RULER to assign relative scores to each trajectory
    #     judged_group = await ruler_score_group(group, "openai/o4-mini", debug=True)
    #     judged_groups.append(judged_group)

    #await model.delete_checkpoints()
    await model.train(
        finished_groups,
        config=art.TrainConfig(learning_rate=training_config["learning_rate"]),
        # Lowering the logprob_calculation_chunk_size is a memory saving measure
        # to allow longer sequences (up to 8192 tokens) to be processed on a T4.
        _config={"logprob_calculation_chunk_size": 8},
    )

    print(f"Completed training step {batch.step}")

    # Stop after max_steps for demo purposes (adjust as needed)
    if batch.step >= training_config["max_steps"]:
        break

