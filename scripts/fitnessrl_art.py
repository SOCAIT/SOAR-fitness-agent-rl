# -*- coding: utf-8 -*-
"""FitnessRL-Art.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x5HZHcJ-pOLZuzH524GrxjOF1RnjbZyB

To train this email search agent using LangGraph, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU!

<div class="align-center">
<a href="https://github.com/openpipe/art"><img src="https://github.com/openpipe/art/raw/main/assets/ART_pill.png" height="50"></a>
<a href="https://discord.gg/zbBHRUpwf4"><img src="https://github.com/openpipe/art/raw/main/assets/Discord.png" height="50"></a>
<a href="https://art.openpipe.ai"><img src="https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png" height="50"></a>

Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).

</div>

<a href="https://art.openpipe.ai/"><img src="https://github.com/openpipe/art/raw/main/assets/Header_separator.png" height="5"></a>

**Fitness Agent with LangGraph**

In this notebook, you will be using [ART](https://github.com/openpipe/art) together with [LangGraph](https://langchain-ai.github.io/langgraph/) to train your own Fitness agent from scratch! This implementation demonstrates how to integrate LangGraph's agent framework with ART's training capabilities.

Beginning with a Qwen 2.5 7B base model, you will train it to generate full Nutrition and Workout plans using LangGraph's ReAct agent pattern. You will construct an [agentic environment](#Environment), define a [rollout](#Rollout) using LangGraph, and run a [training loop](#Loop). You will also learn how to use [RULER](#ruler) to judge the quality of the agent's answers and how to build verifiable reward functions.

**RULER**

RULER is a robust technique for evaluating the quality of an agent's answers and training the agent to produce more of its best completions. To learn more about RULER, see the [RULER documentation](https://art.openpipe.ai/fundamentals/ruler).



Now let's get started!
"""

# Commented out IPython magic to ensure Python compatibility.
# This mounts your Google Drive to the Colab VM.
from google.colab import drive
drive.mount('/content/drive')

# TODO: Enter the foldername in your Drive where you have saved the unzipped
FOLDERNAME =  "AI/AgentRL/FitnessRL"
assert FOLDERNAME is not None, "[!] Enter the foldername."

# Now that we've mounted your Drive, this ensures that
# the Python interpreter of the Colab VM can load
# python files from within it.
import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))
# %cd /content/drive/My\ Drive/$FOLDERNAME/
!ls -a

# Commented out IPython magic to ensure Python compatibility.
# #@title ðŸ’¿ Installation
# 
# # Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)
# # Copyright (c) Unsloth contributors.
# # License: GNU LGPL v3.0.
# # Modifications by OpenPipe:
# # - switched to uv
# # - changed vllm/triton pinning logic
# # - added litellm/protobuf pins
# # See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.
# 
# %%capture
# import os
# 
# if "COLAB_" not in "".join(os.environ.keys()):
#     !uv pip install openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets pinecone  --prerelease allow --no-cache-dir
# else:
#     try:
#         import numpy
# 
#         get_numpy = f"numpy=={numpy.__version__}"
#     except:
#         get_numpy = "numpy"
#     try:
#         import subprocess
# 
#         is_t4 = "Tesla T4" in str(subprocess.check_output(["nvidia-smi"]))
#     except:
#         is_t4 = False
#     get_vllm, get_triton = (
#         ("vllm==0.9.2", "triton==3.2.0") if is_t4 else ("vllm", "triton")
#     )
#     !uv pip install --upgrade \
#         openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir
#     !uv pip install -qqq {get_triton}

import os

from dotenv import load_dotenv

load_dotenv()

# Required
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

from google.colab import userdata

# Get the API key from Colab secrets
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
os.environ["WANDB_API_KEY"] = userdata.get('wandb')
os.environ["PINECONE_API_KEY"] = userdata.get('PINECONE_API_KEY')

if not os.environ.get("OPENAI_API_KEY"):
    raise ValueError(
        "OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini."
    )

# Optional
# os.environ["WANDB_API_KEY"] = "YOUR_API_KEY"

if not os.environ.get("WANDB_API_KEY"):
    print("WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.")

import os
import random
import sqlite3
from dataclasses import asdict, dataclass
from datetime import datetime
from textwrap import dedent
from typing import List, Literal, Optional

from datasets import Dataset, Features, Sequence, Value, load_dataset
from pydantic import BaseModel, Field
from tqdm import tqdm

class Scenario(BaseModel):
   question: str
   split: Literal["train", "test"]
   id: str
   daily_cal_target: Optional[int] = None
   daily_prot_target: Optional[int] = None
   daily_carb_target: Optional[int] = None
   daily_fat_target: Optional[int] = None

from typing import Any, Dict, Literal, Union
from pydantic import field_validator



class FinalAnswer(BaseModel):
    answer: Dict[str, Any]

    @field_validator("answer", mode="before")
    @classmethod
    def ensure_dict(cls, v):
            # Unwrap nested FinalAnswer by mistake
            if isinstance(v, FinalAnswer):
                return v.answer
            # Parse JSON string
            if isinstance(v, str):
                try:
                    return json.loads(v)
                except json.JSONDecodeError as e:
                    raise ValueError(f"answer must be a JSON object string or dict; got invalid JSON: {e}")
            # Already a dict
            if isinstance(v, dict):
                return v
            raise TypeError(f"Unsupported type for answer: {type(v).__name__}")


@dataclass
class SearchResult:
    message_id: str
    snippet: str

from datasets import load_dataset

# Load the dataset from the JSONL file
dataset = load_dataset("json", data_files="fitness_scenarios.jsonl")

# Assuming the dataset has a "train" split, you can access it like this:
training_scenarios = dataset["train"]

print("Dataset loaded successfully!")
print(training_scenarios)

training_scenarios[-1]

import json

def one_day_meal_question(example):
    one_day_prompt = "generate a one day meal plan for user that match its macros and diet"
    example['input_question'] = f"{one_day_prompt}"
    return example

def combine_question_and_context(example):
    context_str = json.dumps(example['context'])
    example['question'] = f"{example['input_question']} Context: {context_str}"
    return example

def convert_val_to_test(example):
    if example['split'] == 'val':
       example['split'] = 'test'
    return example

def get_target_nutrition_data(example):
    daily_cal_target = example['context']['daily_cal_target']
    daily_prot_target = example['context']['daily_prot_target']
    daily_carb_target = example['context']['daily_carb_target']
    daily_fat_target = example['context']['daily_fat_target']
    #banned_keywords = example['banned_keywords']

    example['daily_cal_target'] = daily_cal_target
    example['daily_prot_target'] = daily_prot_target
    example['daily_carb_target'] = daily_carb_target
    example['daily_fat_target'] = daily_fat_target
    #example['banned_keywords'] = banned_keywords
    return example

def make_scenario(example):
    scenario = Scenario(
        question=example['question'],
        split=example['split'],
        id=str(example['id']),
        daily_cal_target=example['daily_cal_target'],
        daily_prot_target=example['daily_prot_target'],
        daily_carb_target=example['daily_carb_target'],
        daily_fat_target=example['daily_fat_target'],
        #banned_keywords=example['banned_keywords']
    )
    return scenario
training_scenarios = training_scenarios.map(one_day_meal_question)
training_scenarios = training_scenarios.map(combine_question_and_context)
training_scenarios = training_scenarios.map(convert_val_to_test)
training_scenarios = training_scenarios.map(get_target_nutrition_data)
# training_scenarios = training_scenarios.map(make_scenario)
print(training_scenarios[0]['question'])

scenarios_list = []
for example in training_scenarios:
    scenario = Scenario(
        question=example['question'],
        split=example['split'],
        id=str(example['id']),
        daily_cal_target=example['daily_cal_target'],
        daily_prot_target=example['daily_prot_target'],
        daily_carb_target=example['daily_carb_target'],
        daily_fat_target=example['daily_fat_target'],
        #banned_keywords=example['banned_keywords']
    )
    scenarios_list.append(scenario)

print(f"Created a list of {len(scenarios_list)} Scenario objects.")

from collections import Counter

print(Counter(training_scenarios['split']))

scenarios_list[0]

import art
from art.local import LocalBackend

random.seed(42)

base_model_name = "Qwen/Qwen2.5-7B-Instruct"
#base_model_name= "Qwen/Qwen3-4B-Instruct-2507"

model = art.TrainableModel(
    name="fitness-agent-langgraph-4B-qwen3-001",
    project="fitness-agent-langgraph",
    base_model=base_model_name,
)

model._internal_config = art.dev.InternalModelConfig(
    init_args=art.dev.InitArgs(
        max_seq_length=8192,
    ),
    engine_args=art.dev.EngineArgs(
        enforce_eager=True,
        gpu_memory_utilization=0.8,
    ),
)

# Initialize the server
backend = LocalBackend(
    # Normally we don't want to run the server in-process, but for the output
    # to show up properly on Google Colab we'll enable this.
    in_process=True,
    path="./.art",
)

# Register the model with the local Backend (sets up logging, inference, and training)
await model.register(backend)

RULER_JUDGE_PROMPT = """
You are a strict evaluator for weekly nutrition/workout plans for SyntraFit.

Score each ASSISTANT response on 0â€“1 (decimals allowed) based on:

[Critical]
1) STRUCTURE: Valid JSON structure per task .
2) TARGET FIT: Daily calories & protein within Â±5% of user's target_nutrition_data (if nutrition task).
3) DIETARY RULES: No banned items present (e.g., â€œeggâ€, â€œshellfishâ€) if user disallows.
4) TOOLING LOGIC: The content is plausibly the result of correct tools (recipe_semantic_search for recipes; get_available_exercises for exercises).

[Quality]
6) REALISM: Meals/exercises are realistic, balanced, and varied across the week; sequencing makes sense (snacks vs meals; rest days).
7) CLARITY: Names are specific (e.g., â€œGrilled Salmon & Quinoaâ€ not â€œhealthy bowlâ€).

Return ONLY a float in [0,1]. Heavily penalize if structure or targets are wrong.
"""

MAX_TURNS = 30

PLANNER_PROMPT = f"""
You are a  nutrition planner specialist who creates daily nutrition plans. you must think carefully and give big attention on the macro numbers.

TOOLS YOU CAN USE (names must match exactly):
1) recipe_semantic_search(meal_query)          â€“ Search for real recipes/macros by query.
5) return_final_answer_tool(final_answer, category)        â€“ Return the final answer ( JSON  plan) and the.

ROUTING RULES (VERY IMPORTANT):
- If the user asks for a meal/nutrition plan, you MUST NOT call get_available_exercises or generate_workout_plan_mutation.
- Even if request is for 7-day plan, just create one day meal plan.

EXECUTION POLICY:
- Do not make the user wait. Produce the complete plan in one shot.
- Output only machine-usable content: the result of the mutation call and then a single call to return_final_answer_tool.
- You may take up to {MAX_TURNS} turns to find the answer,

============================================================
NUTRITION PLAN PIPELINE
============================================================

1) PLAN SKELETON
   â€¢ Generate a day meal plan for user. The plans should have meals that fulfil the daily target macros of the user
   â€¢  create a normal  number of meals to satisfy user nutrition targfet and diet (meals can be snacks). Base the count/portions on the user's macro targets.
   â€¢ Use realistic meal names (e.g., \"Grilled Chicken & Rice\", \"Greek Yogurt with Nuts\"). Do not use placeholders.

2) RECIPE LOOKUP (for EVERY meal)
   â€¢ Call recipe_semantic_search with the meal idea.
   â€¢ From the top result, capture:  recipe name
   â€¢ Replace the meal idea with the exact recipe name  from the tool result. use the correct nutrition info for each meal

3) MACRO ADJUSTMENT (per day)
   â€¢ Sum macros for the day.
   â€¢ If totals differ from the user's daily targets, swap recipes or adjust portions (via another recipe_semantic_search if needed)
     until the daily totals are within Â±5% of the user's targets (calories/protein/carbs/fat).
   â€¢ Respect ALL banned keywords/ingredients from context.

4) JSON Meal PLAN (scratch-step)
   â€¢ Build a JSON  matching this schema.
   â€¢ NOTE: Keep JSON valid (no comments). Example structure (values are illustrative):


     {{
       "meals": [
         {{
           "name": "Grilled Chicken & Rice",          // EXACT recipe name from recipe_semantic_search
           "calories": 700,
           "proteins": 45,
           "carbs": 60,
           "fats": 20,
           "sequence": 1,  // indicates the time of day (e.g., breakfast, lunch, dinner)
         }},
         ... more meals ...
       ]
     }}



   â€¢ Ensure each day's summed macros are within Â±5% of the targets.

5) IF YOU REACHED MAX_TURNS and you have not find a final answer then you have to return a final answer with all your info  you gathered and know
6) TOOL CALLS (Nutrition)
   â€¢ Call generate_nutrition_plan_mutation with the FULL mutation string as the ONLY argument.
   â€¢ Then call return_final_answer_tool with:
     - answer = the EXACT JSON response from the backend (stringified if needed)

"""

!pip install pinecone

from pinecone import Pinecone
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

recipe_index_name = "syntrafit-recipes"
exercise_index_name = "syntrafit-exercises"

recipe_index = pc.Index(recipe_index_name)
exercise_index = pc.Index(exercise_index_name)

def extract_meal_names(data):
    data = data['result']

    return [hit["fields"]["name"] for hit in data["hits"] if "fields" in hit and "name" in hit["fields"]]

 # Search the dense index
results = recipe_index.search(
          namespace="syntrafit",
          query={
              "top_k": 2,
              "inputs": {
                  'text': " Chicken and rice healthy"
              }
          },
          rerank={
          "model": "bge-reranker-v2-m3",
          "top_n": 2,
          "rank_fields": [ "name"]
    },
      )

print(results)

extract_meal_names(results)

import json
import art
from art.rewards import ruler_score_group
import uuid

import weave
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from litellm import acompletion
from tenacity import retry, stop_after_attempt
from art.langgraph import init_chat_model


if os.getenv("WANDB_API_KEY", ""):
    weave.init(model.project, settings={"print_call_link": False})


class NutritionJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    score: float = Field(description="Score between 0 and 1.")


@retry(stop=stop_after_attempt(3))
async def nutrition_judge_score(scenario, answer):


    # # 2) RULER judge
    # judged = await ruler_score_group(
    #     group, "openai/o4-mini",
    #     system_instruction=RULER_JUDGE_PROMPT, debug=False
    # )

    messages = [
        {"role": "system", "content": RULER_JUDGE_PROMPT},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"

                f"Assistant Answer: {answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model="openai/gpt-4.1",
        messages=messages,
        response_format=NutritionJudgeResponse,
    )

    # Access attributes directly instead of using .get()
    return NutritionJudgeResponse(reasoning=response.choices[0].message.content.reasoning, score=response.choices[0].message.content.score)

import json

def get_payload(obj):
    """Return a dict payload from FinalAnswer/str/dict."""
    # unwrap FinalAnswer wrapper
    if hasattr(obj, "answer"):
        obj = obj.answer

    # parse JSON string
    if isinstance(obj, str):
        try:
            obj = json.loads(obj)
        except json.JSONDecodeError:
            # return sentinel dict so schema clearly fails
            return {"_error": "invalid_json_string", "_raw": obj}

    # pydantic compatibility
    if hasattr(obj, "model_dump"):
        obj = obj.model_dump()
    elif hasattr(obj, "dict"):
        obj = obj.dict()

    return obj if isinstance(obj, dict) else {"_error": f"unexpected_type:{type(obj).__name__}", "_raw": str(obj)}

from src.helpers import get_exercise_list_for_prompt
from src.env.verifiers_utils import verify_nutrition_plan, verify_workout_plan, verify_nutrition_schema, verify_meal_plan_schema, verify_daily_meal_plan_macros, is_valid_json
from src.data_utils.create_synthetic_data import data


# def is_valid_json(data):
#     """
#     Check if data is valid JSON (either already parsed dict or parseable string).
#     Returns (bool, parsed_data_or_error_msg)
#     """
#     if isinstance(data, dict):
#         return float(True), data

#     if isinstance(data, str):
#         try:
#             parsed = json.loads(data)
#             return float(True), parsed
#         except json.JSONDecodeError as e:
#             return False, f"JSON parse error: {e.msg} at position {e.pos}"

#     return float(False), f"Invalid input type: {type(data).__name__}"

import json
from functools import wraps
from datetime import datetime

import uuid

import weave
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from litellm import acompletion
from tenacity import retry, stop_after_attempt
from art.langgraph import init_chat_model

import art

if os.getenv("WANDB_API_KEY", ""):
    weave.init(model.project, settings={"print_call_link": False})

# MAX_TURNS = 20

class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None

class FitnessScenario(BaseModel):
    step: int
    scenario: Scenario


@weave.op
async def rollout(model: art.Model, fitness_scenario: FitnessScenario) -> ProjectTrajectory:
    scenario = fitness_scenario.scenario

    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
        metadata={
            "scenario_id": str(scenario.id),
            "step": fitness_scenario.step,
        },
    )

    system_prompt = dedent(
        PLANNER_PROMPT
    )

    # Store final answer in trajectory
    final_answer = None

    # Target Nutrition
    daily_cal_target = scenario.daily_cal_target
    daily_prot_target = scenario.daily_prot_target
    daily_carb_target = scenario.daily_carb_target
    daily_fat_target = scenario.daily_fat_target


    # def log_tool(tool_name):
    #     def decorator(fn):
    #         @wraps(fn)
    #         def wrapper(*args, **kwargs):
    #             call = {
    #                 "tool": tool_name,
    #                 "ts": datetime.utcnow().isoformat(),
    #                 "args": args, "kwargs": kwargs
    #             }
    #             print(f"[TOOL START] {tool_name} args={kwargs}")
    #             traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"start": call})})
    #             try:
    #                 out = fn(*args, **kwargs)
    #                 print(f"[TOOL END] {tool_name} result_preview={str(out)[:400]}")
    #                 traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"end": {**call, "result": out}})})
    #                 return out
    #             except Exception as e:
    #                 print(f"[TOOL ERROR] {tool_name}: {e}")
    #                 traj.messages_and_choices.append({"role": "tool_log", "content": json.dumps({"error": {**call, "error": str(e)}})})
    #                 raise
    #         return wrapper
    #     return decorator


    def log_tool(tool_name):
        def decorator(fn):
            @wraps(fn)
            def wrapper(*args, **kwargs):
                GREEN = "\033[92m"
                RESET = "\033[0m"

                call = {
                    "tool": tool_name,
                    "ts": datetime.utcnow().isoformat(),
                    "args": args,
                    "kwargs": kwargs
                }

                color_prefix = GREEN if "final_answer" in tool_name else ""
                color_reset = RESET if "final_answer" in tool_name else ""

                print(f"{color_prefix}[TOOL START] {tool_name} args={kwargs}{color_reset}")
                traj.messages_and_choices.append(
                    {"role": "tool_log", "content": json.dumps({"start": call})}
                )

                try:
                    out = fn(*args, **kwargs)
                    print(f"{color_prefix}[TOOL END] {tool_name} result_preview={str(out)[:400]}{color_reset}")
                    traj.messages_and_choices.append(
                        {"role": "tool_log", "content": json.dumps({"end": {**call, 'result': out}})}
                    )
                    return out
                except Exception as e:
                    print(f"\033[91m[TOOL ERROR] {tool_name}: {e}\033[0m")
                    traj.messages_and_choices.append(
                        {"role": "tool_log", "content": json.dumps({"error": {**call, "error": str(e)}})}
                    )
                    raise
            return wrapper
        return decorator

    @tool
    @log_tool("recipe_semantic_search")
    def recipe_semantic_search(meal_query: str, k: int = 5) -> str:
      """Search the recipe index for the most similar recipes to the query."""
      # Search the dense index
      results = recipe_index.search(
          namespace="syntrafit",
          query={
              "top_k": 2,
              "inputs": {
                  'text': meal_query
              }
          }
      )

      results = extract_meal_names(results)

      print(results)

      return results


    @tool
    @log_tool("return_final_answer_tool")
    def return_final_answer_tool(answer: str) -> dict:
        """Return the final answer (daily meal plan) in the correct format """
        nonlocal final_answer
        payload = get_payload(answer)          # <-- normalize here
        final_answer = FinalAnswer(answer=payload)
        return final_answer.model_dump()


    # Create LangGraph tools
    tools = [  recipe_semantic_search, return_final_answer_tool]

    # Pass the local path to the model
    chat_model = init_chat_model(f"{model.name}", temperature=1.0)


    # Create the LangGraph ReAct agent
    react_agent = create_react_agent(chat_model, tools)
    print("LangGraph agent created!")
    print(react_agent)

    try:
        # Run the agent
        config = {
            "configurable": {"thread_id": str(uuid.uuid4())},
            "recursion_limit": MAX_TURNS,
        }

        print("Human Question:", scenario.question )



        # Run the agent to get the final result
        res =await react_agent.ainvoke(
            {
                "messages": [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=scenario.question),
                ]
            },
            config=config,
        )


        print("rollout_2")
        print(res)

        # Check if we got a final answer
        if final_answer:
            print("Got final answer!")
            print(final_answer)

            payload = get_payload(final_answer)    # <-- normalize again defensively
            print(payload)                         # should print a dict, not a string

            # Calculate the total reward
            total_reward = 0.0
            traj.final_answer = final_answer
            print(final_answer.answer)

            # Score the trajectory
            # schema_score, info = verify_nutrition_schema(final_answer)
            schema_score, info = verify_meal_plan_schema(payload)
            print("Nutrition Schema score:", schema_score, "info:", info)


            # nutrition_score, info = verify_nutrition_plan(
            # final_answer,
            # daily_cal_target=daily_cal_target,
            # # daily_carb_target=daily_carb_target,
            # # daily_fat_target=daily_fat_target,
            # daily_prot_target=daily_prot_target,
            # )

            nutrition_score, info = verify_daily_meal_plan_macros(
                payload,
                daily_cal_target=daily_cal_target,
                # daily_carb_target=daily_carb_target,
                # daily_fat_target=daily_fat_target,
                daily_prot_target=daily_prot_target,
            )

            print("Nutrition score:", nutrition_score, "info:", info)

            # json_score, info = is_valid_json(final_answer.answer)
            # print("JSON score:", json_score, "info:", info)

            total_reward = 0.75*nutrition_score + 0.25* schema_score #+ 0.15*json_score
            print("Total Verifiable Reward:", total_reward)

            # traj.final_answer = final_answer
            # # Score the trajectory
            # correctness_judge_response = await nutrition_judge_score(
            #     scenario, traj.final_answer.answer
            # )

            # judge_score= correctness_judge_response.score
            # print("Judge score:", judge_score)
            # total_reward = total_reward + judge_score

            # judge_reasoning = correctness_judge_response.reasoning
            # print("Judge reasoning:", judge_reasoning)
            # traj.metrics["judge_reasoning"] = judge_reasoning



            random_noise = random.random()
            traj.reward = total_reward + random_noise * 0.005
            print(f"Total reward: {traj.reward}")
            traj.metrics["correct"] = total_reward

        # traj.reward= -1.0 + random_noise * 0.005
        # traj.metrics["correct"] = traj.reward


    except Exception as e:
        print(f"Error running LangGraph agent: {e}")
        # Add error information to trajectory
        traj.messages_and_choices.append(
            {"role": "assistant", "content": f"Error: {str(e)}"}
        )

    return traj


print("LangGraph rollout function defined!")

# Training configuration
from art.utils import iterate_dataset
from art.langgraph import wrap_rollout

training_config = {
    "groups_per_step": 2,
    "num_epochs": 30,
    "rollouts_per_group": 4,
    "learning_rate": 1e-5,
    "max_steps": 30,
}

# Use iterate_dataset with real training scenarios (similar to train.py)
training_iterator = iterate_dataset(
    scenarios_list,  # Use real scenarios from Hugging Face , training_scenarios
    groups_per_step=training_config["groups_per_step"],
    num_epochs=training_config["num_epochs"],
    #initial_step=await model.get_step(),
)

for batch in training_iterator:
    print(
        f"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}"
    )
    print(f"Batch contains {len(batch.items)} scenarios")

    # Create trajectory groups for this batch (similar to train.py)
    groups = []
    for scenario_data in batch.items:
        # # Ensure the id is a string when creating FitnessScenario
        # scenario = Scenario(
        #     question=scenario_data['question'],
        #     split=scenario_data['split'],
        #     id=str(scenario_data['id']) # Convert id to string here
        # )
        scenario = scenario_data
        print(scenario)
        groups.append(
            art.TrajectoryGroup(
                (
                    wrap_rollout(model, rollout)(
                        #model, FitnessScenario(step=batch.step, scenario=scenario.model_dump())
                        model, FitnessScenario(step=batch.step, scenario=scenario.model_dump())
                    )
                    for _ in range(training_config["rollouts_per_group"])
                )
            )
        )
    print("Group:", groups[0])
    # Gather all trajectory groups
    finished_groups = await art.gather_trajectory_groups(
        groups,
        pbar_desc="gather",
        max_exceptions=training_config["rollouts_per_group"] * len(batch.items),
    )

    print("Finished groups:", finished_groups)

    # judged_groups = []
    # for group in finished_groups:
    #     # Use RULER to assign relative scores to each trajectory
    #     judged_group = await ruler_score_group(group, "openai/o4-mini", debug=True)
    #     judged_groups.append(judged_group)

    #await model.delete_checkpoints()
    await model.train(
        finished_groups,
        config=art.TrainConfig(learning_rate=training_config["learning_rate"]),
        # Lowering the logprob_calculation_chunk_size is a memory saving measure
        # to allow longer sequences (up to 8192 tokens) to be processed on a T4.
        _config={"logprob_calculation_chunk_size": 8},
    )

    print(f"Completed training step {batch.step}")

    # Stop after max_steps for demo purposes (adjust as needed)
    if batch.step >= training_config["max_steps"]:
        break